{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da61b5bc-788f-4151-9ee5-966fc66490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to batch load data into big query as a pipeline from the activity-data-p1 folder. In this folder, the new batches of collected data can be added and it will automatically be added\n",
    "# to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de7b0f-722c-4e54-a1f0-84763359cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: int, b: int, year: int, month: int, day: int, hour: int, generation_solar: int, generation_wind_offshore: int, generation_wind_onshore: int, forecast_solar_day_ahead: int, forecast_wind_onshore_day_ahead: int, price_day_ahead: float, price_actual: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "# Add extra types to make sure loading data to GBQ goes as expected.\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType, IntegerType, FloatType\n",
    "from time import sleep\n",
    "import pyspark.sql.functions as F\n",
    "import datetime as dt\n",
    "#import spark.implicits._\n",
    "#import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreaming pipeline 1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"de_jads_temp_snellejassie\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"a\", IntegerType(), True),\n",
    "         StructField(\"b\", IntegerType(), True),\n",
    "         StructField(\"year\", IntegerType(), True),\n",
    "         StructField(\"month\", IntegerType(), True),\n",
    "         StructField(\"day\", IntegerType(), True),\n",
    "         StructField(\"hour\", IntegerType(), True),\n",
    "         StructField(\"generation_solar\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_offshore\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_onshore\", IntegerType(), True),\n",
    "         StructField(\"forecast_solar_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"forecast_wind_onshore_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"price_day_ahead\", FloatType(), True),\n",
    "         StructField(\"price_actual\", FloatType(), True)\n",
    "         ])\n",
    "\n",
    "# Read data that is added to the activity-data-p1 folder for this first pipeline, where the data in this folder consists out of multiple files. And data can periodically be added to this\n",
    "# folder in order to keep batch uploading it.\n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .csv(\"/home/jovyan/data/activity-data/activity-data-p1\")\n",
    "# Print the structure of the data to see whether all types are correctly assigned.\n",
    "print(sdf)\n",
    "\n",
    "activityCounts = sdf.groupBy(\"a\", \"b\", F.col('year'), F.col('month'), F.col('day'), F.col('hour'),\n",
    "                            F.col('price_actual'), F.col('price_day_ahead'), F.col('generation_wind_offshore'), F.col('generation_wind_onshore'),\n",
    "                            F.col('forecast_solar_day_ahead'), F.col('forecast_wind_onshore_day_ahead'), F.col(\"generation_solar\")).count()\n",
    "\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'de2022-assignment2-fresh.pipelines.tablepipeline1') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# Use your gcp bucket name. \n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "activityQuery = activityCounts.writeStream.outputMode(\"complete\") \\\n",
    "                    .trigger(processingTime = '2 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "try:\n",
    "    activityQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    activityQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stopped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7dac8d31-e15f-45cd-b2e6-75a9fdfce07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
