{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c721d9-6e03-47f1-9940-09c060a18db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 8_1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3a6499a-efcf-4674-963d-1cc0fec09460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ts: timestamp, generation_solar: int, generation_wind_offshore: int, generation_wind_onshore: int, forecast_solar_day_ahead: int, forecast_wind_onshore_day_ahead: int, price_day_ahead: float, price_actual: float]\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "|         4271.0|    1|\n",
      "|          714.0|    1|\n",
      "|         5138.0|    1|\n",
      "|         7408.0|    3|\n",
      "|         1033.0|    1|\n",
      "|         5343.0|    2|\n",
      "|         4393.0|    1|\n",
      "|         6801.0|    1|\n",
      "|         3675.0|    1|\n",
      "|         5925.0|    1|\n",
      "|         5750.0|    3|\n",
      "|         3599.0|    2|\n",
      "|         9317.0|    1|\n",
      "|         5758.0|    2|\n",
      "|         3451.0|    2|\n",
      "|         4446.0|    1|\n",
      "|         7702.0|    1|\n",
      "|         6663.0|    1|\n",
      "|         2976.0|    1|\n",
      "|         1575.0|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "|          714.0|    1|\n",
      "|        13994.0|    1|\n",
      "|        14204.0|    1|\n",
      "|        10358.0|    1|\n",
      "|         3675.0|    2|\n",
      "|         5750.0|    3|\n",
      "|         9531.0|    1|\n",
      "|         4446.0|    1|\n",
      "|         6663.0|    1|\n",
      "|         2976.0|    1|\n",
      "|        10415.0|    1|\n",
      "|         5138.0|    1|\n",
      "|         1033.0|    1|\n",
      "|        11033.0|    1|\n",
      "|         2098.0|    2|\n",
      "|        10237.0|    1|\n",
      "|         5925.0|    2|\n",
      "|         6471.0|    1|\n",
      "|         3599.0|    3|\n",
      "|         3451.0|    3|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "|          714.0|    1|\n",
      "|        13994.0|    1|\n",
      "|        14204.0|    1|\n",
      "|         2679.0|    1|\n",
      "|        10376.0|    1|\n",
      "|        10358.0|    1|\n",
      "|         3675.0|    4|\n",
      "|         6833.0|    1|\n",
      "|         5750.0|    5|\n",
      "|         9531.0|    2|\n",
      "|         4446.0|    3|\n",
      "|         6663.0|    1|\n",
      "|         2976.0|    3|\n",
      "|        10415.0|    1|\n",
      "|         5138.0|    1|\n",
      "|         1033.0|    1|\n",
      "|        11033.0|    1|\n",
      "|         2098.0|    2|\n",
      "|        10237.0|    1|\n",
      "|         5925.0|    2|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|price_day_ahead|count|\n",
      "+---------------+-----+\n",
      "|        13994.0|    1|\n",
      "|        10358.0|    1|\n",
      "|         3675.0|    6|\n",
      "|         6833.0|    2|\n",
      "|         9531.0|    2|\n",
      "|         5138.0|    1|\n",
      "|         1033.0|    1|\n",
      "|         6631.0|    1|\n",
      "|         5925.0|    2|\n",
      "|         6471.0|    2|\n",
      "|         3599.0|    5|\n",
      "|         9024.0|    1|\n",
      "|         1575.0|    3|\n",
      "|         3238.0|    5|\n",
      "|         4618.0|    3|\n",
      "|         4271.0|    4|\n",
      "|         6130.0|    1|\n",
      "|         9317.0|    1|\n",
      "|         5039.0|    1|\n",
      "|         8984.0|    3|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType, IntegerType, FloatType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreaming\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"ts\", TimestampType(), True),\n",
    "         StructField(\"generation_solar\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_offshore\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_onshore\", IntegerType(), True),\n",
    "         StructField(\"forecast_solar_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"forecast_wind_onshore_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"price_day_ahead\", FloatType(), True),\n",
    "         StructField(\"price_actual\", FloatType(), True)\n",
    "         ])\n",
    "\n",
    "# Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .csv(\"/home/jovyan/data/activity-data\")\n",
    "print(sdf)\n",
    "# Do a calculation\n",
    "activityCounts = sdf.groupBy(\"price_day_ahead\").count()\n",
    "\n",
    "# Write to a sink - here, the output is memory (only for testing). The query name (i.e., activity_counts) will be the Spark SQL table name.\n",
    "activityQuery = activityCounts.writeStream.queryName(\"price_day_ahead_counts\") \\\n",
    "                    .format(\"memory\").outputMode(\"complete\") \\\n",
    "                    .start()\n",
    "# Testing \n",
    "for x in range(10):\n",
    "    spark.sql(\"SELECT * FROM price_day_ahead_counts\").show()\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b09f2702-b4ce-435d-9e3e-497e021a212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8792bba-515c-4e55-b8da-1224e4b5cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 8_2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a13746ed-479f-4bce-8545-1b01391aedea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ts: timestamp, generation_solar: int, generation_wind_offshore: int, generation_wind_onshore: int, forecast_solar_day_ahead: int, forecast_wind_onshore_day_ahead: int, price_day_ahead: float, price_actual: float]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o455.start.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:276)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:195)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:48)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:279)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:326)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:427)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:406)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 37\u001b[0m\n\u001b[1;32m     32\u001b[0m activityCounts \u001b[38;5;241m=\u001b[39m sdf\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_day_ahead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Write to a sink - here, the output is memory (only for testing). The query name (i.e., activity_counts) will be the Spark SQL table name.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m activityQuery \u001b[38;5;241m=\u001b[39m \u001b[43mactivityCounts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice_day_ahead_counts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 37\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     activityQuery\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o455.start.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:113)\n\tat org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:276)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:195)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:48)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:279)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:326)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:427)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:406)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType, IntegerType, FloatType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreaming\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"ts\", TimestampType(), True),\n",
    "         StructField(\"generation_solar\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_offshore\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_onshore\", IntegerType(), True),\n",
    "         StructField(\"forecast_solar_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"forecast_wind_onshore_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"price_day_ahead\", FloatType(), True),\n",
    "         StructField(\"price_actual\", FloatType(), True)\n",
    "         ])\n",
    "\n",
    "# Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .csv(\"/home/jovyan/data/activity-data\")\n",
    "\n",
    "print(sdf)\n",
    "# Do a calculation\n",
    "activityCounts = sdf.groupBy(\"price_day_ahead\").count()\n",
    "\n",
    "# Write to a sink - here, the output is memory (only for testing). The query name (i.e., activity_counts) will be the Spark SQL table name.\n",
    "activityQuery = activityCounts.writeStream.queryName(\"price_day_ahead_counts\") \\\n",
    "                    .format(\"console\").outputMode(\"complete\") \\\n",
    "                    .start()\n",
    "try:\n",
    "    activityQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    activityQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stopped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d5a62a5-7793-48c3-a2c3-69cc24ee500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da61b5bc-788f-4151-9ee5-966fc66490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 8_3 batches to big query\n",
    "\n",
    "# Laat deze een tijdje runnen en daarna stoppen, dan is het te zien in Google Cloud bij Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de7b0f-722c-4e54-a1f0-84763359cf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ts: timestamp, generation_solar: int, generation_wind_offshore: int, generation_wind_onshore: int, forecast_solar_day_ahead: int, forecast_wind_onshore_day_ahead: int, price_day_ahead: float, price_actual: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType, IntegerType, FloatType\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"SparkStreaming\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# We need to set the following configuration whenever we need to use GCS.\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "bucket = \"de_jads_temp_snellejassie\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"ts\", TimestampType(), True),\n",
    "         StructField(\"generation_solar\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_offshore\", IntegerType(), True),\n",
    "         StructField(\"generation_wind_onshore\", IntegerType(), True),\n",
    "         StructField(\"forecast_solar_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"forecast_wind_onshore_day_ahead\", IntegerType(), True),\n",
    "         StructField(\"price_day_ahead\", FloatType(), True),\n",
    "         StructField(\"price_actual\", FloatType(), True)\n",
    "         ])\n",
    "\n",
    "# Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .csv(\"/home/jovyan/data/activity-data/activity-data-p1\")\n",
    "print(sdf)\n",
    "# .option(\"maxBytesPerTrigger\", \"65536b\")\n",
    "# Do a calculation\n",
    "activityCounts = sdf.groupBy(\"price_actual\", \"generation_solar\", \"generation_wind_offshore\", \"generation_wind_onshore\"\n",
    "                                           , \"forecast_solar_day_ahead\", \"forecast_wind_onshore_day_ahead\", \"price_day_ahead\").count()\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'de2022-assignment2-fresh.energyprediction.energytable12') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "# Write to a sink - here, the output is written to a Big Query Table\n",
    "# Use your gcp bucket name. \n",
    "# ProcessingTime trigger with two-seconds micro-batch interval\n",
    "activityQuery = activityCounts.writeStream.outputMode(\"complete\") \\\n",
    "                    .trigger(processingTime = '2 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "try:\n",
    "    activityQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    activityQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stopped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dac8d31-e15f-45cd-b2e6-75a9fdfce07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
